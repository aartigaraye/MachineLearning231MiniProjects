---
title: "Lab 1"
subtitle: "PSTAT 131/231"
author: Aarti Garaye
output: pdf_document
urlcolor: blue
linkcolor: red
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Setup

### R and RStudio

All work in this course, including homework, labs, and the final project, will be conducted using *R* and *RStudio*. I understand that not all students will already be familiar with *R*, so I don't expect that everyone starts out at the same coding level. The instructional team, myself included, are here to help!

First, go to <https://www.r-project.org/> and click *Download R*. Select a CRAN mirror link. Do this **EVEN IF** you already have R installed on your machine. If you have a previous R installation, re-downloading R will *update* your copy of R to the most recent version, which often fixes many small problems.

Next, go to <https://www.rstudio.com/products/rstudio/download/> and download the **free** version of RStudio Desktop. We will almost always open and use RStudio to interact with R.

You will be working with RStudio a lot, and you'll have time to learn most of the bells and whistles RStudio provides. Think about RStudio as your "workbench". Keep in mind that RStudio is MORE than plain R. RStudio is an environment that makes it easier to work with R, while handling many of the little tasks than can be a hassle.

At this point, your TA will give a brief overview of the RStudio default four-pane layout and demonstrate how to change fonts, settings, etc.

#### Getting Help with R

Much of the time we spend using R involves interacting with functions. For example, to find the average of three numbers, we can call the `mean()` function:

```{r}
mean(c(1, 2, 3))
```

Each function in R has its own set of arguments and possible values that these arguments accept. You will often need to look up a specific function or one of its arguments -- very often! The good news is, there is a lot of R documentation out there, and it's fairly easy to get help.

To get help about `mean()`, you can uncomment (delete the `#`) and run either of these lines:

```{r}
# ?mean
# help(mean)
```

Or simply open your Web browser and do a search for something like `R function mean help`.

#### The *tidyverse* and *tidymodels*

Throughout this course, in the homework and labs, we'll spend a lot of time using the [*tidyverse*](https://www.tidyverse.org/) and [*tidymodels*](https://www.tidymodels.org/). These are two collections of R packages that not only work together very well but also are relatively easy to use. The [*tidyverse*](https://www.tidyverse.org/) makes a lot of data manipulation, exploring, and visualizing much simpler, and [*tidymodels*](https://www.tidymodels.org/) has provided a framework that allows users to fit machine learning models in R more easily than ever before.

I recommend loading the following packages for all your homework assignments and for your final project. We also load a few extra packages here for use later on.

What do you think `tidymodels_prefer()` might do? Try looking it up to find out (or asking your TA)!

```{r}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(ggthemes)
tidymodels_prefer()
```

Recall that the first time you use the packages, you'll need to install them using `install.packages()`; make sure to install any packages **outside** of an .Rmd file, though, because including that command in an .Rmd will prevent the file from knitting. Speaking of .Rmd files:

### Update an .Rmd

Markdown is the language R uses to create and update documents. If you write in a Markdown file within a **code chunk**, as shown below, that text will be processed and run like R code. If you write *outside* of the code chunks, like here, that text will not be run and will appear as text. You can format it as usual, include headings, etc.!

.Rmd files are a special type of file, referred to as a dynamic document, that allows users to combine narrative (text) with R code. Because you will be turning in all your work for this course as .Rmd files and their knitted .html or .pdf file(s), it is important that you quickly become familiar with this resource.

Try updating the code in the following code chunk. Assign `2+2` to another object, called `y`. `<-` is the assignment operator in R, commonly read as "gets."

```{r}
# This is a code chunk!
# Any uncommented text in here will be run as R code.
# For example:
x <- seq(1, 10, 1)
x
```

Take some time and work through the Markdown tutorial here: [www.markdown-tutorial.com](www.markdown-tutorial.com).

In Markdown, code chunks can have specific options set for them; you can also set the options for chunks in the entire document. At the top of this .Rmd, you'll see a code chunk with `opts_chunk$set()`. Any options you set inside that function will apply to all code chunks in the document. I recommend you set the options used in this file for all your assignments, along with the options at the **very** top of the document --- `toc: true`, `toc_float: true`, and `code_folding: show`. You can go further and customize Markdown files as much as you like, but that's not required.

### Creating an R Project

I **strongly** recommend working in R within the context of [an R project](https://bookdown.org/ndphillips/YaRrr/projects-in-rstudio.html). It sounds complicated or unnecessary at first, but an R project -- which is essentially a special working directory, designated with an (automatically created) `.Rproj` file -- can make your life **much** easier, especially when working on your final project.

Your TA can now go over how to create a new project.

Working in an R project automatically sets your working directory to that project folder, rather than whatever your computer's default working directory is. That means you can readily access other .R scripts, photos, data files, etc. simply by putting them in your project folder, without having to write out lengthy file paths.

## Basics of Data Processing

Now we'll take some time to go over some of the basic tools for managing data via the *tidyverse*. There are many more functions that you might find useful, and you can read more about them in [R for Data Science](https://r4ds.had.co.nz/), if you're interested.

First, you'll need to install and load some packages. These include, but are not limited to: `tidyverse`, `tidymodels`, and `ISLR`. Make sure to install each of these using the `install.packages()` function and load them with `library()`.

```{r}
library(tidyverse)
library(tidymodels)
library(ISLR)
```

Some packages include datasets when they are loaded. Set `eval = TRUE` and knit your .Rmd to run the following code chunk:

```{r, eval = FALSE}
mpg
```

Run `?mpg` to learn more about this data set.

There are five key `tidyverse` functions, or "verbs." We'll go through each of them briefly with the `mpg` data set. All of these functions work similarly; their first argument is a data frame, subsequent arguments describe operations on the data frame, and the function's result is a new data frame.

### Select observations by their value: `filter()`

Say that you are interested in selecting only those rows in `mpg` that represent Audi compact cars. The easiest way to select them is:

```{r}
mpg %>% 
  filter(class == "compact" & manufacturer == "audi")
```

The above code takes the `mpg` data set and pipes it into `filter()`. The pipe symbol is `%>%`; a shortcut for typing it is Cmd+Shift+M on Macs, or Cntrl+Shift+M on Windows.

If you want to store the result of your filtering, you need to assign it to an object:

```{r}
filtered_mpg <- mpg %>% 
  filter(class == "compact" & manufacturer == "audi")
```

You can use the classic comparison operators -- `!=` for not equal to, `==` for equal to, `>`, etc. They can also be used in combination with Boolean operators, as demonstrated above; `&` for "and", `|` for "or", and `!` for "not."

\newpage

#### Activities:

On your own, find ways to filter the `flights` data set from the `nycflights13` package to achieve each of the following:

-   Had an arrival delay of two or more hours
-   Flew to Houston (IAH or HOU)
-   Were operated by United, American, or Delta
-   Departed in summer (July, August, and September)
-   Arrived more than two hours late, but didn't leave late
-   Were delayed by at least an hour, but made up over 30 minutes in flight
-   Departed between midnight and 6am (inclusive)

\textbf{Solution:} Follow the code below. Before that, it is important to note the format of the `flights` dataset. You can follow this [link](https://nycflights13.tidyverse.org/reference/flights.html) but I have added it in this lab for convinience.

Format
Data frame with columns

* year, month, day: Date of departure.

* dep_time, arr_time: Actual departure and arrival times (format HHMM or HMM), local tz.

* sched_dep_time, sched_arr_time: Scheduled departure and arrival times (format HHMM or HMM), local tz.

* dep_delay, arr_delay: Departure and arrival delays, in minutes. Negative times represent early departures/arrivals.

* carrier: Two letter carrier abbreviation. See [airlines](https://nycflights13.tidyverse.org/reference/airlines.html) to get name.

* flight: Flight number.

* tailnum: Plane tail number. See [planes](https://nycflights13.tidyverse.org/reference/planes.html) for additional metadata.

* origin, dest: Origin and destination. See [airports](https://nycflights13.tidyverse.org/reference/flights.html#:~:text=and%20destination.%20See-,airports,-for%20additional%20metadata) for additional metadata.

* air_time: Amount of time spent in the air, in minutes.

* distance: Distance between airports, in miles.

* hour, minute: Time of scheduled departure broken into hour and minutes.

* time_hour: Scheduled date and hour of the flight as a POSIXct date. Along with origin, can be used to join flights data to [weather](https://nycflights13.tidyverse.org/reference/weather.html) data.

```{r}
library(nycflights13)

# Had an arrival delay of two or more hours
filtered_flights_arr_delay <- flights %>%
  filter(arr_delay >= 120) # since arrival delay is in minutes in the dataset

# Flew to Houston (IAH or HOU)
filtered_flights_to_houston <- flights %>%
  filter(dest == "IAH" | dest == "HOU")

# Operated by United, American, or Delta
filtered_flights_un_am_dt <- flights %>%
  filter(carrier == "UA" | carrier == "AA" | carrier == "DL")

# Departed in summer (July, August, and September) 
filtered_flights_dept_summer <- flights %>%
  filter(month == 7 | month == 8 | month == 9)

# Arrived more than two hours late, but didn’t leave late
filtered_flights_arrlate_deptnotlate <- flights %>%
  filter(arr_delay > 120 & dep_delay <= 0)

# Were delayed by at least an hour, but made up over 30 minutes in flight
filtered_flights_delayOnePlusHour_made30mins <- flights %>%
  filter(dep_delay >= 60 & (dep_delay - arr_delay) > 30)

# Departed between midnight and 6am (inclusive)
filtered_flights_midnight_6 <- flights %>%
  filter(dep_time >= 0 & dep_time <= 600) # dep_time, arr_time are in HHMM or HMM format
```

\newpage

### Select specific variables or columns by their names: `select()`

Often in machine learning, we end up working with very large data sets that have a lot of columns. The `mpg` data set is pretty small, but we can still practice with it.

We can select the `year`, `hwy`, and `class` variables and store them in a new object, `mpg_small`, by:

```{r}
mpg_small <- mpg %>% 
  select(year, hwy, class)
```

For a shortcut, when working with large data frames, we can use `(year:class)` or `-(year:class)` to select or de-select all columns including them and between them, respectively.

Note that we use the `head()` function here so that only a few rows of the resulting tibble are displayed when we knit to .html.

```{r}
mpg %>% select(year:class) %>% 
  head()

mpg %>% select(-(year:class)) %>% 
  head()
```

The tidyverse includes a number of helper functions that can be used inside `select()`, like `starts_with()`, etc. You can see more of them with `?select`.

\newpage

#### Activities

On your own, working with the `flights` data:

-   Find as many ways as you can to select `dep_time`, `dep_delay`, `arr_time`, and `arr_delay`.

-   What happens if you include the name of a variable multiple times in a `select()` call?

\textbf{Solution:} For this activity we will be trying different methods to hopefully get the same result. Follow the code below:

```{r, warning=FALSE}
library(waldo)

way1 <- flights %>%
  select(dep_time, dep_delay, arr_time, arr_delay)

way2 <- flights %>%
  select(starts_with("dep"), starts_with("arr"))

way3 <- flights %>%
  select(contains("dep_"), contains("arr_"))

arr_dep_col <- c("dep_time", "dep_delay", "arr_time", "arr_delay")
way4 <- flights %>%
  select(all_of(arr_dep_col))

compare(way1, way2, way3, way4)
```

Let's try if we include the name of a variable multiple times in a `select()` call:

Case 1: Just one variable multiple times
```{r}
library(knitr)
multiple_select <- flights %>%
  select(dep_time, dep_time, dep_time)
kable(head(multiple_select), 
      caption = "Result of selecting just one variable multiple times.")
```
If it's just one variable that is repeated multiple times, it gives just that one variable column. 

Case 2: One variable multiple times and one more variable
```{r}
multiple_select2 <- flights %>%
  select(dep_time, dep_time, dep_delay)
kable(head(multiple_select2), 
      caption = "Result of selecting one variable multiple times and one more variable")
```
It gives both variables just once.

Case 3: Repeating one variable, not in order
```{r}
multiple_select3 <- flights %>%
  select(dep_time, dep_delay, arr_time, arr_delay, dep_delay)
kable(head(multiple_select3), 
      caption = "result of repeating one variable, not in order")
```
As we can see if we choose one variable multiple times in the `select()` it doesn't change anything. This gives us a lot more ways to perform the select command in previous parts. 

\newpage

### Create or add new variables: `mutate()`

Besides selecting existing columns, it's often useful to add new columns that are functions of existing columns. That's the job of `mutate()`.

`mutate()` always adds new columns at the end of your dataset, so we'll use `select()` to reorder the columns and put the new ones at the front. `everything()` is a helper function to grab all the other variables.

We can add a new column that has the value `0` for cars manufactured before $2000$ and `1` for those manufactured after $2000$ with the following code. Variables set up in this way are "dummy-coded."

```{r}
mpg %>% 
  mutate(after_2k = if_else(year <= 2000, 0, 1)) %>% 
  select(after_2k, year, everything()) %>% 
  head()
```

You can see an overview of a number of useful variable creation functions here: <https://r4ds.had.co.nz/transform.html#mutate-funs>.

For an alternative to `mutate()` when you only want to retain the newly created variables, not all variables, use `transmute()`:

```{r}
transmute(mpg,
  after_2k = if_else(year <= 2000, 0, 1)) %>% 
  head()
```

\newpage

#### Activities

On your own, working with the `flights` data:

-   Currently `dep_time` and `sched_dep_time` are convenient to look at, but hard to compute with because they're not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.

-   What does 1:3 + 1:10 return? Why do you think it returns this?

\textbf{Solution:} Currently `dep_time` and `sched_dep_time` are in the format HHMM or HMM local time zone. To convert them to continuous numbers it would be best to write them as minutes since they would be continuous. These would be the number of minutes after midnight since we already have HHMM and HMM as military style. We have to careful around the format.  Follow the code below:

```{r}
cont_time_flights <- flights %>%
  mutate(
    dep_time_mins = (dep_time %/% 100) * 60 + (dep_time %% 100),
    sched_dep_time_mins = (sched_dep_time %/% 100) * 60 + (sched_dep_time %% 100)
  )
```

The table above shows the continuous time conversion of departure time and scheduled departure times to be continuous numbers that is the time in minutes which would be easier to compute.

```{r}
1:3 + 1:10
```
This is because R is thinking we are adding two sequences, 1:3 = 1 2 3 and 1:10 = 1 2 3 4 5 6 7 8 9 10 and the first place values are added together, second places together, and third place together. However, the first sequence is only 3 places long, so the fourth place in the second sequence is added to the first place value in the shorter sequence and fifth to second and so on and so forth. See the image below:
![](/Users/aarti/Downloads/lab1seqadd.jpeg)
\newpage

### Create grouped summaries of data frames: `summarise()`

The last key verb function is `summarise()`. It's most useful when combined with `group_by()`, so that it produces a summary for each level or value of a variable/group. Notice what happens if used without grouping:

```{r}
mpg %>% 
  summarise(avg_hwy = mean(hwy))
```

This value represents the average highway mileage across *all cars in the data frame*. We can see immediately that, while this has certainly reduced the size of the data frame, it's not very useful. Instead, we might prefer the average highway mileage by class of car, or by manufacturer. We can view these, and even `arrange()` by highway mileage:

```{r}
mpg %>% 
  group_by(class) %>% 
  summarise(avg_hwy = mean(hwy)) %>% 
  arrange(avg_hwy)

mpg %>% 
  group_by(manufacturer) %>% 
  summarise(avg_hwy = mean(hwy)) %>% 
  arrange(avg_hwy)
```

The following code finds the average highway mileage by manufacturer, counts the number of cars produced by each manufacturer, and prints the top 10 manufacturers with largest numbers of cars, arranged by mileage:

```{r}
mpg %>% 
  group_by(manufacturer) %>% 
  summarise(avg_hwy = mean(hwy),
            count = n()) %>% 
  filter(count >= 9) %>% 
  arrange(avg_hwy)
```

It's not demonstrated here, but you can also use other verbs like `mutate()` and `filter()` in conjunction with `group_by()`. Use `ungroup()` when you want to return to ungrouped data.

## Exploratory Data Analysis

The last section of this lab will guide you through practicing exploratory data analysis on the dataset `diamonds` (contained in the `ggplot2` package).

### Diamonds

We'll start with the `diamonds` data set. First, let's take a look at the first few lines of it, to get a feel for it:

```{r}
diamonds %>% 
  head()
```

Think about which of these variables we might want to predict with a machine learning model. `price` makes intuitive sense; it's not something we can simply directly measure from a diamond, and we would likely be very interested in knowing how much a given diamond is worth.

\newpage

#### Activities:

- How many observations are there in `diamonds`?
- How many variables? Of these, how many are features we could use for predicting `price`?

\textbf{Solution:} There are multiple ways of looking at how many observations are there in a table, but I want to manually look at them using the `view()` function.

```{r}
view(diamonds)
```
There are 53940 observations in this dataset. Another way of doing this is just counting the rows because this is a tidy data meaning that each row is an observation and each column is a variable. Thus, the number of observations can be just counted using the `nrow()` function.
```{r}
nrow(diamonds)
```
There are 53940 observations in the diamonds dataset. 

As mentioned above, this is a tidy data set. The number of variables is just the number of columns and we can get that using the `ncol()` function. By just viewing the data we can manually count as well.
```{r}
ncol(diamonds)
```
There are 10 columns, there names are:
```{r}
colnames(diamonds)
```
Out of these 10, I don't know what "x", "y", and "z" are so I would say evey other varibale is important in predicting price. We can look at the correlation matrix to see which of these have the most impact on the price. For now, I think "carat," "cut," "color," "clarity," "depth," and "table" would have a considerable impact on the price of the diamond. 

\newpage


Run `?diamonds` and look at the variable definitions.

First, let's make a correlation matrix to see which continuous variables are correlated with `price`. See example code below:

```{r}
diamonds %>% 
  select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = FALSE, 
           method = 'color')
```

Take a moment and look at the arguments in the `corrplot()` function. What does each one do? What happens if you change `diag` to `TRUE` and `method` to `'square'`?

\textbf{Solution:} From the correlation matrix above we can see price is highly positively correlated with carat, x, y, and z which is surprising because we don't know what x, y, and z is. The color, clarity, and cut are all missing from this lower triangle of the correlation matrix. Let's see what happens if we change the `diag` to be `TRUE` and `method` to be `square`. After looking up the diamonds dataset descriptions, `x` is the length in mm, `y` is width in mm, and `z` is depth in mm whereas `depth` is the total depth percentage. Thus, the bigger the diamond the more expensive it is which coroborate how these are highly positively correlated.

```{r}
diamonds %>% 
  select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = TRUE, 
           method = 'square')
```

When we change the `diag` to be `TRUE` we say that we want to have the correlation coefficients for the variables with themselves. So now we have price with price being highly correlated and all the diagonal entries are highly positively correlated. Changing the `method` to be `square` we just changed how much correlation looks like. Correlation that is closer to zero would be smaller size squares and highly correlated would have big filled squares with the appropriate color according to the spectrum in the bottom.

\newpage

#### Activities:

- Which features are positively correlated with `price`? Do these make sense?
- Are any features negatively correlated with `price`?
- Which features are correlated with *each other*? Why do you think this might be?

Let's make a boxplot of the distribution of `price` per level of `cut` and `color`, to see if there appears to be a relationship between it and these predictors. I looked up the description of the diamonds data set. 

```{r}
diamonds %>% 
  ggplot(aes(x = price, y = reorder(cut, price), fill = color)) + 
  geom_boxplot() +
  labs(y = "Cut", x = "Price") +
  theme_bw()
```

\textbf{Solution:} From the correlation matrix `carat`, `x`, `y`, `z`, and a little bit of `table` are positively related to `price`. Carat makes most sense because higher carat diamonds are more expensive. The x, y, and z makes sense as well because they are the dimensions of the diamond, the bigger the diamond the more expensive it is. 

None of the features are negatively correlated with `price` because diamonds are a commodity, they will never be cheap and nothing can make the price go down, there can be features that make it more expensive. 

`table` and `depth` are negatively correlated with each other. This makes sense because as you increase the depth of a diamond the area you can see from the top of the diamond goes down. Furthermore `x`, `y`, and `z` are all correlated with each other and with `carat` because the dimensions of the diamond and the total carat weight must depend on one another. 

\newpage

#### Activities:

- What do you learn from this plot about the relationship between `price`, `cut`, and `color`?

- Refer back to the definitions of the variables in `?diamonds`. Does anything you learned surprise you?

Since `J` is the worst color for diamonds, why do you think they tend to cost more?

Let's take a look at the relationship between `color` and `carat` to explore further. Remember from the correlation plot that `carat` is highly positively correlated with `price`.

```{r}
diamonds %>% 
  ggplot(aes(x = carat, y = reorder(color, carat))) + 
  geom_boxplot() +
  theme_bw() +
  labs(x = "Carat", y = "Color")
```

\textbf{Solution:} In each cut, the j color in a diamond is most expensive. The priciest diamond is color j premium cut. What surprised me a little was the j color in ideal cut, it was more expensive than all the j colors in other cuts. Looking at all the boxplot it seems like the most expensive cut would be the premium cut diamonds, and then maybe the very good cut. It's harder to see since there is a lot going on in that graph.

It surprised me that J is the worst color and D is the best color and that I1 is the worst clarity and IF is the best clarity when the worst color is the most expensive on according to the graph above. The carat of J diamonds in the data set are mostly greater than 1, referring to the graph of box plots of color vs carat. Furthermore, a [diamond expert blog](https://www.pricescope.com/education/diamond-color/j-color-diamond) said,

"Cut quality influences everything: Light escaping through the pavilion or bouncing around inside the diamond illuminates body tone. Light returning with intensity to the viewer’s eyes can reduce the appearance of color. A J color diamond may have color reduction or improvement depending on its cut quality." 

Generally a better color diamond would be more expensive if every other varible is held constant in the comparison. However, in this dataset the J color diamonds tend to have a higher carat weight, which makes them more expensive. In the correlation matrix above we didn't have the correlation between `color` and `carat` since it was lower triangle and `color` is not numeric. To make sense of the graph a lot more we need to figure out a way of finding the correlation between `color` and `carat.`

One way we could try is making them numeric just so see whether there is a relationship between color and carat. Since it is going from D-J we can make D = 7, E = 6,...,J = 1 and see when the color increases, meaning it improves a color grade, is there a correlation between that and carat. However, `color` is not numeric and the correlation matrix only works for the numeric type of variables. Fortunately, we are given the boxplot representing `color` and `carat`. From the boxplot graph above we know that the median J color diamonds have a higher carat weight and the D color diamonds have lower median carat weight. Thus, for this dataset, the worst color is more expensive because it most probably has higher carat weight. Since `carat` and `price` are highly correlated, it influences the price much more than `color` does.

\newpage


#### Activities:

- Explain why lower-quality colors tend to cost more.

Now we'll assess the distribution of our outcome variable `price`. Let's make a histogram:

```{r}
diamonds %>% 
  ggplot(aes(x = price)) +
  geom_histogram(bins = 60) +
  theme_bw()
```

Notice that we increased the number of bins here; this allows us to get a more fine-grained picture of the distribution. `price` is positively skewed, meaning that much of the mass of its distribution is at the lower end, with a long tail to the right. Most diamonds in the data set are worth less than $\$10,000$.

\textbf{Solution:} Lower quality colors tend to cost more because the carat weight for them in this dataset is on the higher end. This is not the reason for most of the diamonds, but in this dataset, when we group by the color, the lower quality diamonds tend to have a higher carat weight compared to the lower quality color diamonds, on average. 

\newpage

#### Activities:

- Create a single plot to visualize the relationship between `cut`, `carat`, and `price`.

\textbf{Solution:} We can create a scatter plot with different attributes to visualize the relationship between `cut`, `carat`, and `price`. 

```{r}
ggplot(diamonds, aes(x = carat, y = reorder(cut, carat))) + 
  geom_boxplot() +
  theme_bw() +
  labs(x = "Carat", y = "Color")
```


## Data Splitting

Now we're going to walk through the process of splitting the `diamonds` data set into two, a training set and a test set. Note that in the future, after we discuss the concept of resampling, we'll use a resampling technique called cross-validation, but for now, we'll work with the entire training set.

We also could have performed this split prior to doing exploratory data analysis, and in future we'll split data first. That's arguably better practice because it means we will have never encountered the test observations before we fit a final model to them.

The textbook(s) describe a way to split data using base R functions, but `tidymodels` makes the process a lot easier.

We set a seed first because the splitting process is random. If we don't set a seed, each time we re-run the code we'll get a new random split, and the results will not be identical.

In general, set a seed to whatever number you like. People often use birthdates, anniversaries, or lucky numbers, etc. Just make sure you remember the number, because you'll need to set the seed to that number to reproduce your split in future.

```{r}
set.seed(3435)

diamonds_split <- initial_split(diamonds, prop = 0.80,
                                strata = price)
diamonds_train <- training(diamonds_split)
diamonds_test <- testing(diamonds_split)
```

#### Activities:

- How many observations are now in the training and testing sets, respectively? Report the exact number, not a proportion.

- What do you think the `strata = price` argument does? Take a guess, then use `?initial_split` to verify.
